{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.1-1.2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51bcd30187c5f165"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T09:58:40.697464200Z",
     "start_time": "2024-05-12T09:58:31.611440200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3c2d855ac4dfc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.01\n",
    "momentum=0.9\n",
    "weight_decay=1e-04\n",
    "epochs = 25\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # to use the GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T09:58:41.700746400Z",
     "start_time": "2024-05-12T09:58:41.687745700Z"
    }
   },
   "id": "8f2620e5222e739b",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data preparation\n",
    "\n",
    "Dataset loading, validation splitting code for CIFAR10."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8f66b29ae2fbb54"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "# create a split for train/validation. We can use early stop\n",
    "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000])  # train (40000 images) e validation (10000 images)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2,\n",
    "                                          drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2,\n",
    "                                          drop_last=False)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2,\n",
    "                                          drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T09:58:43.852101700Z",
     "start_time": "2024-05-12T09:58:41.704749100Z"
    }
   },
   "id": "8858e8eb39cc42f5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training and evaluation Functions \n",
    "\n",
    "Training, evaluation, and plotting code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da3c52139fb5d10a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, criterion, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xs)\n",
    "        loss = criterion(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()   \n",
    "        losses.append(loss.item())\n",
    "    # print('Train Loss: {:.6f}'.format(np.mean(losses)))\n",
    "    wandb.log({'Train Loss': np.mean(losses)})\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, criterion, device='cpu', val=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "        xs = xs.to(device)\n",
    "        output = model(xs)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        test_loss += criterion(output, ys.to(device)).item()\n",
    "        gts.append(ys)\n",
    "        predictions.append(preds.detach().cpu().numpy())\n",
    "    mode = \"Val\" if val else \"Test\"\n",
    "    # print('\\{} set: Average loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "    #     mode,\n",
    "    #     test_loss/len(dl), accuracy_score(np.hstack(gts), np.hstack(predictions))))   \n",
    "    # Return accuracy score and classification report.\n",
    "    wandb.log({'Test Loss': test_loss/len(dl), 'Test Accuracy': round(accuracy_score(np.hstack(gts), np.hstack(predictions)), 2)})\n",
    "    \n",
    "\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(losses_and_accs):\n",
    "    losses = [x for (x, _) in losses_and_accs]\n",
    "    accs = [x for (_, x) in losses_and_accs]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accs)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')\n",
    "    \n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T09:58:49.495565700Z",
     "start_time": "2024-05-12T09:58:47.375199100Z"
    }
   },
   "id": "9bc785716e684c16",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Here there is a *simple* Multilayer Perceptron to classify the 10 classes of CIFAR10. Here we exploit the training pipeline above. This training pipeline monitors the loss and accuracy on the training and validation sets for every epoch using weights and biases."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1381b2a01c15ca1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30134e6feeecdb30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Dumb_MLP(nn.Module):\n",
    "    def __init__(self, dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim*2)\n",
    "        self.fc3 = nn.Linear(dim*2, dim*4)\n",
    "        self.fc4 = nn.Linear(dim*4, dim*8)\n",
    "        self.fc5 = nn.Linear(dim*8, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T10:37:03.698584500Z",
     "start_time": "2024-05-11T10:37:03.625679700Z"
    }
   },
   "id": "6e23967ff09e288",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3235efadf6f64eef97256297bd55f254"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\eleri\\PycharmProjects\\Lab1_DLA\\wandb\\run-20240511_123717-zptn3qk9</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9' target=\"_blank\">MLP</a></strong> to <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x1cf6784ba90>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Lab1-DLA\",\n",
    "    name=\"MLP\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"MLP\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T10:37:28.906715Z",
     "start_time": "2024-05-11T10:37:17.786706700Z"
    }
   },
   "id": "e4977b15039871f9",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP trainable parameters:  374730\n"
     ]
    }
   ],
   "source": [
    "model_mlp = Dumb_MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model_mlp.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "wandb.watch(model_mlp, log='all')\n",
    "print('MLP trainable parameters: ', count_trainable_parameters(model_mlp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T10:37:31.008460600Z",
     "start_time": "2024-05-11T10:37:30.962331900Z"
    }
   },
   "id": "fc4c9f5592f42b39",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 625/625 [00:29<00:00, 21.26it/s]\n",
      "Training epoch 2: 100%|██████████| 625/625 [00:25<00:00, 24.89it/s]\n",
      "Training epoch 3: 100%|██████████| 625/625 [00:22<00:00, 27.79it/s]\n",
      "Training epoch 4: 100%|██████████| 625/625 [00:22<00:00, 28.17it/s]\n",
      "Training epoch 5: 100%|██████████| 625/625 [00:22<00:00, 28.09it/s]\n",
      "Training epoch 6: 100%|██████████| 625/625 [00:21<00:00, 29.10it/s]\n",
      "Training epoch 7: 100%|██████████| 625/625 [00:22<00:00, 27.66it/s]\n",
      "Training epoch 8: 100%|██████████| 625/625 [00:24<00:00, 25.34it/s]\n",
      "Training epoch 9: 100%|██████████| 625/625 [00:21<00:00, 29.31it/s]\n",
      "Training epoch 10: 100%|██████████| 625/625 [00:21<00:00, 28.54it/s]\n",
      "Training epoch 11: 100%|██████████| 625/625 [00:22<00:00, 28.02it/s]\n",
      "Training epoch 12: 100%|██████████| 625/625 [00:22<00:00, 28.22it/s]\n",
      "Training epoch 13: 100%|██████████| 625/625 [00:22<00:00, 28.31it/s]\n",
      "Training epoch 14: 100%|██████████| 625/625 [00:21<00:00, 28.64it/s]\n",
      "Training epoch 15: 100%|██████████| 625/625 [00:23<00:00, 26.62it/s]\n",
      "Training epoch 16: 100%|██████████| 625/625 [00:34<00:00, 17.91it/s]\n",
      "Training epoch 17: 100%|██████████| 625/625 [00:43<00:00, 14.28it/s]\n",
      "Training epoch 18: 100%|██████████| 625/625 [00:44<00:00, 13.95it/s]\n",
      "Training epoch 19: 100%|██████████| 625/625 [00:34<00:00, 18.14it/s]\n",
      "Training epoch 20: 100%|██████████| 625/625 [00:32<00:00, 18.97it/s]\n",
      "Training epoch 21: 100%|██████████| 625/625 [00:34<00:00, 18.14it/s]\n",
      "Training epoch 22: 100%|██████████| 625/625 [00:36<00:00, 17.34it/s]\n",
      "Training epoch 23: 100%|██████████| 625/625 [00:32<00:00, 19.32it/s]\n",
      "Training epoch 24: 100%|██████████| 625/625 [00:36<00:00, 17.23it/s]\n",
      "Training epoch 25: 100%|██████████| 625/625 [00:33<00:00, 18.66it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec7eb06522c84bb1b3a25ee2bfdc32b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇█▇▇█████████</td></tr><tr><td>Test Loss</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▁</td></tr><tr><td>Train Loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.5</td></tr><tr><td>Test Loss</td><td>1.38887</td></tr><tr><td>Train Loss</td><td>1.36913</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">MLP</strong> at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/zptn3qk9</a><br/> View project at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240511_123717-zptn3qk9\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    train_epoch(model_mlp, trainloader, optimizer, criterion, epoch, device=device)\n",
    "    evaluate_model(model_mlp, valloader, criterion, device=device)\n",
    "    \n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T10:56:33.374669900Z",
     "start_time": "2024-05-11T10:37:32.246464500Z"
    }
   },
   "id": "64d94b8a77bb9e7",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "Now we repeat the same experiments with **Convolutional** Neural Networks using the same pipeline as above. The objective is to show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "The convolutional neural network is defined from its blocks which can be both Basic Blocks and Bottleneck that are the typical building blocks for ResNets.\n",
    "\n",
    "![\\label{ResBlocks}](images/img.png)\n",
    "\n",
    "The `skip`  parameter allows the skip connections. In this way we can compare the same architecture with or without residual connections."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37900fb0b8bb754d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Basic code for 3x3 and 1x1 convolutions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2d51b5ad7069ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T09:58:58.560391900Z",
     "start_time": "2024-05-12T09:58:58.531389100Z"
    }
   },
   "id": "296c5714a05a7374",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolutional Blocks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e2ebc81d6fbd856"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        skip: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.skip = skip\n",
    "        self.downsample = None\n",
    "        if self.skip:\n",
    "            self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.skip:\n",
    "            if self.downsample is not None:\n",
    "                identity = self.downsample(x)\n",
    "            out += identity\n",
    "        \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T11:58:56.314289200Z",
     "start_time": "2024-05-12T11:58:56.289254300Z"
    }
   },
   "id": "b18a04c144a37901",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Main model backbone\n",
    "This module allows to choose the number of blocks of type [BasicBlock, Bottleneck] per layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dd761191cb9b21f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        skip: bool = False,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.skip = skip\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            norm_layer(self.inplanes),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.layer1 = self._make_layer(24, layers[0])\n",
    "        self.layer2 = self._make_layer(48, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(96, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(96 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * BasicBlock.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * BasicBlock.expansion, stride),\n",
    "                norm_layer(planes * BasicBlock.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            BasicBlock(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer, self.skip\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                BasicBlock(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                    skip=self.skip\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:00:14.785962800Z",
     "start_time": "2024-05-12T12:00:14.763956700Z"
    }
   },
   "id": "e5e66727513d3058",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _convnet(\n",
    "    layers: List[int],\n",
    "    num_classes: int,\n",
    "    groups: int = 1,\n",
    "    width_per_group: int = 64,\n",
    "    skip: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> ConvNet:\n",
    "\n",
    "    model = ConvNet(layers, num_classes, False, groups, width_per_group, skip, **kwargs)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:00:15.838811600Z",
     "start_time": "2024-05-12T12:00:15.773602900Z"
    }
   },
   "id": "ffdfddf7bfff9455",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Models\n",
    "Here we define the models for our experiments."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "659d292452c5e718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def small_convnet(num_classes) -> ConvNet:\n",
    "    return _convnet([1, 1, 1], num_classes, groups=1, width_per_group=64, skip=False)\n",
    "def small_resnet(num_classes) -> ConvNet:\n",
    "    return _convnet([1, 2, 2], num_classes, groups=1, width_per_group=64, skip=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:00:17.276509900Z",
     "start_time": "2024-05-12T12:00:17.230512600Z"
    }
   },
   "id": "c9e6e9c8af226200",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=96, out_features=10, bias=True)\n",
      ")\n",
      "ConvNet trainable parameters:  178026\n"
     ]
    }
   ],
   "source": [
    "convnet = small_convnet(10)\n",
    "print(convnet)\n",
    "print('ConvNet trainable parameters: ', count_trainable_parameters(convnet))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:00:18.774285200Z",
     "start_time": "2024-05-12T12:00:18.700284700Z"
    }
   },
   "id": "316aef2d480449d3",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=96, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = small_resnet(10)\n",
    "print(resnet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:17:41.471710200Z",
     "start_time": "2024-05-12T12:17:41.432193800Z"
    }
   },
   "id": "35b842f974952e1c",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d8a7ed3c132457c98210d08bc9e2a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\eleri\\PycharmProjects\\Lab1_DLA\\wandb\\run-20240512_135933-4lptbsua</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua' target=\"_blank\">CNN</a></strong> to <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x269a157f400>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Lab1-DLA\",\n",
    "    name='CNN',\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T11:59:44.540605300Z",
     "start_time": "2024-05-12T11:59:33.182998500Z"
    }
   },
   "id": "e1018855caefaaf7",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 625/625 [00:22<00:00, 27.20it/s]\n",
      "Training epoch 2: 100%|██████████| 625/625 [00:21<00:00, 28.85it/s]\n",
      "Training epoch 3: 100%|██████████| 625/625 [00:21<00:00, 28.51it/s]\n",
      "Training epoch 4: 100%|██████████| 625/625 [00:28<00:00, 21.86it/s]\n",
      "Training epoch 5: 100%|██████████| 625/625 [00:25<00:00, 24.95it/s]\n",
      "Training epoch 6: 100%|██████████| 625/625 [00:25<00:00, 24.04it/s]\n",
      "Training epoch 7: 100%|██████████| 625/625 [00:24<00:00, 25.28it/s]\n",
      "Training epoch 8: 100%|██████████| 625/625 [00:24<00:00, 25.77it/s]\n",
      "Training epoch 9: 100%|██████████| 625/625 [00:25<00:00, 24.58it/s]\n",
      "Training epoch 10: 100%|██████████| 625/625 [00:23<00:00, 26.33it/s]\n",
      "Training epoch 11: 100%|██████████| 625/625 [00:24<00:00, 25.17it/s]\n",
      "Training epoch 12: 100%|██████████| 625/625 [00:22<00:00, 27.50it/s]\n",
      "Training epoch 13: 100%|██████████| 625/625 [00:22<00:00, 27.55it/s]\n",
      "Training epoch 14: 100%|██████████| 625/625 [00:22<00:00, 27.24it/s]\n",
      "Training epoch 15: 100%|██████████| 625/625 [00:22<00:00, 27.91it/s]\n",
      "Training epoch 16: 100%|██████████| 625/625 [00:23<00:00, 27.06it/s]\n",
      "Training epoch 17: 100%|██████████| 625/625 [00:22<00:00, 27.33it/s]\n",
      "Training epoch 18: 100%|██████████| 625/625 [00:22<00:00, 27.18it/s]\n",
      "Training epoch 19: 100%|██████████| 625/625 [00:21<00:00, 28.84it/s]\n",
      "Training epoch 20: 100%|██████████| 625/625 [00:21<00:00, 28.48it/s]\n",
      "Training epoch 21: 100%|██████████| 625/625 [00:22<00:00, 27.56it/s]\n",
      "Training epoch 22: 100%|██████████| 625/625 [00:25<00:00, 24.16it/s]\n",
      "Training epoch 23: 100%|██████████| 625/625 [00:22<00:00, 28.22it/s]\n",
      "Training epoch 24: 100%|██████████| 625/625 [00:25<00:00, 24.64it/s]\n",
      "Training epoch 25: 100%|██████████| 625/625 [00:21<00:00, 28.52it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2085a602601a48aeaa3dd601fed76fc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▃▄▃▅▅▆▆▆▆▇▇▇▇▇▇█▇█████▇█</td></tr><tr><td>Test Loss</td><td>█▆▅▆▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁</td></tr><tr><td>Train Loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.82</td></tr><tr><td>Test Loss</td><td>0.53215</td></tr><tr><td>Train Loss</td><td>0.4579</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">CNN</strong> at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/4lptbsua</a><br/> View project at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240512_135933-4lptbsua\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convnet = small_convnet(10).to(device)\n",
    "optimizer = torch.optim.SGD(convnet.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "wandb.watch(convnet, log='all')\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_epoch(convnet, trainloader, optimizer, criterion, epoch, device=device)\n",
    "    evaluate_model(convnet, valloader, criterion, device=device)\n",
    "    \n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:16:06.844481400Z",
     "start_time": "2024-05-12T12:00:25.300838600Z"
    }
   },
   "id": "3f669fb80dd3510c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0341bd2feca44d36b5da498f3147c9fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\eleri\\PycharmProjects\\Lab1_DLA\\wandb\\run-20240512_141754-5nbs4qwb</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb' target=\"_blank\">ResNet</a></strong> to <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x26b76bc8ac0>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Lab1-DLA\",\n",
    "    name='ResNet',\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"ResNet\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:18:04.401265100Z",
     "start_time": "2024-05-12T12:17:54.360766700Z"
    }
   },
   "id": "b4c7566c82be8737",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 625/625 [00:26<00:00, 23.37it/s]\n",
      "Training epoch 2: 100%|██████████| 625/625 [00:28<00:00, 21.68it/s]\n",
      "Training epoch 3: 100%|██████████| 625/625 [00:27<00:00, 22.43it/s]\n",
      "Training epoch 4: 100%|██████████| 625/625 [00:30<00:00, 20.26it/s]\n",
      "Training epoch 5: 100%|██████████| 625/625 [00:27<00:00, 22.59it/s]\n",
      "Training epoch 6: 100%|██████████| 625/625 [00:27<00:00, 22.99it/s]\n",
      "Training epoch 7: 100%|██████████| 625/625 [00:28<00:00, 21.75it/s]\n",
      "Training epoch 8: 100%|██████████| 625/625 [00:28<00:00, 22.22it/s]\n",
      "Training epoch 9: 100%|██████████| 625/625 [00:27<00:00, 22.49it/s]\n",
      "Training epoch 10: 100%|██████████| 625/625 [00:27<00:00, 22.99it/s]\n",
      "Training epoch 11: 100%|██████████| 625/625 [00:28<00:00, 21.72it/s]\n",
      "Training epoch 12: 100%|██████████| 625/625 [00:27<00:00, 22.80it/s]\n",
      "Training epoch 13: 100%|██████████| 625/625 [00:26<00:00, 23.44it/s]\n",
      "Training epoch 14: 100%|██████████| 625/625 [00:26<00:00, 23.16it/s]\n",
      "Training epoch 15: 100%|██████████| 625/625 [00:27<00:00, 22.66it/s]\n",
      "Training epoch 16: 100%|██████████| 625/625 [00:26<00:00, 23.24it/s]\n",
      "Training epoch 17: 100%|██████████| 625/625 [00:27<00:00, 23.09it/s]\n",
      "Training epoch 18: 100%|██████████| 625/625 [00:27<00:00, 22.60it/s]\n",
      "Training epoch 19: 100%|██████████| 625/625 [00:27<00:00, 22.64it/s]\n",
      "Training epoch 20: 100%|██████████| 625/625 [00:28<00:00, 21.88it/s]\n",
      "Training epoch 21: 100%|██████████| 625/625 [00:29<00:00, 21.12it/s]\n",
      "Training epoch 22: 100%|██████████| 625/625 [00:28<00:00, 21.73it/s]\n",
      "Training epoch 23: 100%|██████████| 625/625 [00:30<00:00, 20.53it/s]\n",
      "Training epoch 24: 100%|██████████| 625/625 [00:28<00:00, 21.66it/s]\n",
      "Training epoch 25: 100%|██████████| 625/625 [00:28<00:00, 22.26it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65000b596a1b49ad9cc89fe9e6f5b16d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇████</td></tr><tr><td>Test Loss</td><td>█▆▅▄▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁</td></tr><tr><td>Train Loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.86</td></tr><tr><td>Test Loss</td><td>0.4149</td></tr><tr><td>Train Loss</td><td>0.32385</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">ResNet</strong> at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA/runs/5nbs4qwb</a><br/> View project at: <a href='https://wandb.ai/unifi_ai/Lab1-DLA' target=\"_blank\">https://wandb.ai/unifi_ai/Lab1-DLA</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20240512_141754-5nbs4qwb\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet = small_resnet(10).to(device)\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "wandb.watch(resnet, log='all')\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_epoch(resnet, trainloader, optimizer, criterion, epoch, device=device)\n",
    "    evaluate_model(resnet, valloader, criterion, device=device)\n",
    "    \n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T12:45:28.510310200Z",
     "start_time": "2024-05-12T12:28:01.630768500Z"
    }
   },
   "id": "7c70121abee92893",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "The figure below shows the Validation accuracy during training of ResNet, ConvNet and MLP. The performances of the MLP are much worse than the other two models. The ResNet which is deeper offers a gain in performance of 4 percentage points at the end of training. \n",
    "<img src=\"images/Val_acc.png\" alt=\"drawing\" width=\"1200\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfe3aeb4bcce547"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff91b2681b0c2f29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
